{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Daily Challenge: Simplified Self-Attention Explained"
      ],
      "metadata": {
        "id": "2-PWp9i4sSsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What Will You Create\n",
        "* A deeper understanding of the self-attention process.\n",
        "* A clear mental model of how self-attention calculates relationships within a sequence.\n",
        "* An explanation of each step in the provided self attention code.\n"
      ],
      "metadata": {
        "id": "UzKXa0DosfX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Simplified self-attention\n",
        "\n",
        "We implement a simplified variant of self-attention, free from any trainable waights. The goal of this section is to illustrate a few key consetps in self attention before adding trainable weights.\n",
        "\n",
        "* Load Input Tensor (Word Embeddings):\n",
        "  * Start with numerical representations of words (embeddings) because neural networks process numbers. This is the input data our self-attention mechanism will work on."
      ],
      "metadata": {
        "id": "2LcKrPGusxUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn # Import the torch.nn module\n"
      ],
      "metadata": {
        "id": "VMksi1kPJXrd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UZvXie2Jr2aD"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor(\n",
        "[\n",
        "    [0.43, 0.15, 0.89], # your\n",
        "    [0.55, 0.87, 0.66], # journey\n",
        "    [0.57, 0.85, 0.64], # starts\n",
        "    [0.22, 0.58, 0.33], # with\n",
        "    [0.77, 0.25, 0.10], # one\n",
        "    [0.05, 0.80, 0.55] # step\n",
        "]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Select a Query Vector:\n",
        "  * In self-attention, we compare each word (vector) against others to understand their relationships. The “query” is the word we’re currently focusing on.\n",
        "* 1.1  Computing Attention Weights for Inputs[2]:\n",
        "  * 1.1.1 Attention Score:\n",
        "  \n",
        "    The dot product measures how similar two vectors are. Higher scores indicate greater similarity. We’re finding how relevant each word is to our “query” word."
      ],
      "metadata": {
        "id": "OouRlksMtRtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty tensor to store the attention scores\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "\n",
        "# List of words corresponding to each embedding\n",
        "words = [\"your\", \"journey\", \"starts\", \"with\", \"one\", \"step\"]\n",
        "\n",
        "# Select \"starts\" as the query word (Q)\n",
        "query = inputs[2]\n",
        "\n",
        "# Compute attention scores using the dot product between each word and the query\n",
        "attn_scores_2 = torch.matmul(inputs, query)\n",
        "\n",
        "# Print attention scores for each word\n",
        "for i, score in enumerate(attn_scores_2):\n",
        "    print(f\"Attention score between '{words[i]}' and 'starts': {score.item()}\")\n",
        "\n",
        "# Display final attention scores\n",
        "print(\"Attention scores:\", attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NNAM_-Yt2Y3",
        "outputId": "809cd16a-0feb-4839-9b7d-927233a67a62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention score between 'your' and 'starts': 0.9422000050544739\n",
            "Attention score between 'journey' and 'starts': 1.4754000902175903\n",
            "Attention score between 'starts' and 'starts': 1.4570000171661377\n",
            "Attention score between 'with' and 'starts': 0.8295999765396118\n",
            "Attention score between 'one' and 'starts': 0.715399980545044\n",
            "Attention score between 'step' and 'starts': 1.0605000257492065\n",
            "Attention scores: tensor([0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  *  1.1.2 Attention Weights:\n",
        "        - Softmax transforms the scores into probabilities (attention weights).\n",
        "        - These weights represent how much “attention” each word should receive when we create the context vector."
      ],
      "metadata": {
        "id": "Y7puFfwTIU2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Softmax to transform attention scores into attention weights (probabilities)\n",
        "m = nn.Softmax(dim=-1)  # Use dim=-1 to make it explicit\n",
        "attn_weights_2 = m(attn_scores_2)\n",
        "\n",
        "# Print attention weights\n",
        "print(\"Attention Weights Matrix:\")\n",
        "print(attn_weights_2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7aqnXLQIuDY",
        "outputId": "9be7a06f-b499-4af6-8d9a-64340b0023ee"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights Matrix:\n",
            "tensor([0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 1.1.3 Context Vector:\n",
        "- The context vector is a weighted sum of the input vectors. It represents a refined version of the query, incorporating information from other relevant words."
      ],
      "metadata": {
        "id": "DR0O5insMfjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiply attention weights by the value (which is the same as inputs in this case)\n",
        "context_vectors_2 = torch.matmul(attn_weights_2, inputs)\n",
        "\n",
        "print(\"Context Vectors:\")\n",
        "print(context_vectors_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1BtU6mzMqvj",
        "outputId": "a9fa45f9-8fb6-4aa5-98de-0cd7faa1d88f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vectors:\n",
            "tensor([0.4431, 0.6496, 0.5671])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  *  1.2 Computing Attention Weights for All Inputs:\n",
        "- 1.2.1 Attention Score:\n",
        "        - Extend the process to compute attention scores for every word against every other word in the sequence. This creates a matrix of relationships."
      ],
      "metadata": {
        "id": "ZhALP7-jNyZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty tensor for attention scores (square matrix: num_words x num_words)\n",
        "attn_scores = torch.empty(inputs.shape[0], inputs.shape[0])\n",
        "\n",
        "# Compute dot product between all pairs of words\n",
        "for i, query in enumerate(inputs):\n",
        "    for j, key in enumerate(inputs):\n",
        "        attn_scores[i, j] = torch.dot(query, key)  # Compute similarity between words i and j\n",
        "\n",
        "# Print the attention score matrix\n",
        "print(\"Attention Score Matrix:\")\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3Of5YpaN-v3",
        "outputId": "f3046eea-746c-4033-abc8-31e819af1d56"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Score Matrix:\n",
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 1.2.2 Attention Weights:\n",
        "- Apply softmax across rows to get attention weights for each word, showing its relationship to all others."
      ],
      "metadata": {
        "id": "cO0ztZu-OazN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply softmax along each row (dim=1), so that each word's attention scores sum to 1\n",
        "softmax = nn.Softmax(dim=1)\n",
        "attn_weights = softmax(attn_scores)\n",
        "\n",
        "# Print the attention weight matrix\n",
        "print(\"Attention Weights Matrix:\")\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLc3vl-mOf5L",
        "outputId": "371d2ac5-6a5c-4ce8-b2d1-25f28c5b1585"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights Matrix:\n",
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 1.2.3 All Context Vector:\n",
        "- Generate a context vector for each word, capturing its meaning in the context of the entire sequence."
      ],
      "metadata": {
        "id": "rRAyQMbROxQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the context vectors for each word\n",
        "context_vectors = torch.matmul(attn_weights, inputs)\n",
        "\n",
        "# Print the context vectors\n",
        "print(\"All Context Vectors:\")\n",
        "print(context_vectors)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZVOJTx1O5BG",
        "outputId": "54482629-287a-4843-8dce-c8cd325cb2bf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Context Vectors:\n",
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. The ‘Self’ in Self-Attention¶\n",
        "- In self-attention, the ‘self’ refers to the mechanism’s ability to computer attention weights by relating different positions within a single input sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "12I-GmInRVEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 2.1 Weights Parameters vs Attention Weights:\n",
        "\n",
        "   * Distinguish between learned parameters (weights of the network) and dynamically computed attention weights. This clarifies the different roles they play."
      ],
      "metadata": {
        "id": "TTbidK2FR7nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 2.2 Computing Weight Parameters for Inputs[1]:\n",
        "   * 2.2.1 Initialize the three weight matrices Wq, Wk, Wv:\n",
        "     * Introduce learnable weight matrices (Wq, Wk, Wv) to transform input vectors into queries, keys, and values. This adds flexibility and allows the model to learn complex relationships."
      ],
      "metadata": {
        "id": "VSayaZGiSV2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define the dimensionality of input embeddings\n",
        "d_model = inputs.shape[1]  # In this case, d_model = 3\n",
        "\n",
        "# Initialize learnable weight matrices Wq, Wk, Wv (size 3x3)\n",
        "Wq = torch.randn(d_model, d_model)  # Query weight matrix\n",
        "Wk = torch.randn(d_model, d_model)  # Key weight matrix\n",
        "Wv = torch.randn(d_model, d_model)  # Value weight matrix\n",
        "\n",
        "# Print the weight matrices\n",
        "print(\"Query Weight Matrix (Wq):\")\n",
        "print(Wq)\n",
        "print(\"\\nKey Weight Matrix (Wk):\")\n",
        "print(Wk)\n",
        "print(\"\\nValue Weight Matrix (Wv):\")\n",
        "print(Wv)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCQkx54fShnn",
        "outputId": "6811bf99-e659-4179-b364-8995ddeb1e0d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Weight Matrix (Wq):\n",
            "tensor([[ 0.3367,  0.1288,  0.2345],\n",
            "        [ 0.2303, -1.1229, -0.1863],\n",
            "        [ 2.2082, -0.6380,  0.4617]])\n",
            "\n",
            "Key Weight Matrix (Wk):\n",
            "tensor([[ 0.2674,  0.5349,  0.8094],\n",
            "        [ 1.1103, -1.6898, -0.9890],\n",
            "        [ 0.9580,  1.3221,  0.8172]])\n",
            "\n",
            "Value Weight Matrix (Wv):\n",
            "tensor([[-0.7658, -0.7506,  1.3525],\n",
            "        [ 0.6863, -0.3278,  0.7950],\n",
            "        [ 0.2815,  0.0562,  0.5227]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * 2.2.2 Compute the query, key, and value vectors for inputs[1]:\n",
        "    * These transformations project the input into different “spaces” that emphasize different aspects of the word’s meaning."
      ],
      "metadata": {
        "id": "hMAO2fkmWFQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the query, key, and value vectors for all input words\n",
        "queries = torch.matmul(inputs, Wq)  # Transform input into query space\n",
        "keys = torch.matmul(inputs, Wk)  # Transform input into key space\n",
        "values = torch.matmul(inputs, Wv)  # Transform input into value space\n",
        "\n",
        "# Select \"starts\" (inputs[2]) as the query vector\n",
        "query_1 = queries[1]  # Extract query vector for \"starts\"\n",
        "\n",
        "# Extract the corresponding key and value vectors for \"starts\"\n",
        "key_1 = keys[1]  # Extract key vector for \"starts\"\n",
        "value_1 = values[1]  # Extract value vector for \"starts\"\n",
        "\n",
        "# Print the query, key, and value for \"starts\"\n",
        "print(\"Query Vector for 'journey':\")\n",
        "print(query_1)\n",
        "print(\"\\nKey Vector for 'journey':\")\n",
        "print(key_1)\n",
        "print(\"\\nValue Vector for 'journey':\")\n",
        "print(value_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghWzrs94WQAd",
        "outputId": "33963fbe-d775-4536-c603-97920933605e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Vector for 'journey':\n",
            "tensor([ 1.8430, -1.3271,  0.2715])\n",
            "\n",
            "Key Vector for 'journey':\n",
            "tensor([ 1.7453, -0.3033,  0.1241])\n",
            "\n",
            "Value Vector for 'journey':\n",
            "tensor([ 0.3617, -0.6609,  1.7805])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    * 2.2.3 Compute the Attention Score inputs[1][1] or ω11:\n",
        "      * Calculate the similarity between the transformed query and key."
      ],
      "metadata": {
        "id": "_Kxj76sdYn9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the attention score between \"journey\" and itself (ω₁₁)\n",
        "attn_score_11 = torch.dot(query_1, key_1)\n",
        "\n",
        "# Print the attention score\n",
        "print(\"Attention Score ω₁₁ (between 'journey' and itself):\")\n",
        "print(attn_score_11)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5Pyk3n3b7dD",
        "outputId": "99620e3c-01db-46d8-af3d-ea659a17a083"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Score ω₁₁ (between 'journey' and itself):\n",
            "tensor(3.6527)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 2.2.4 Compute all the Attention Scores for inputs[1]:\n",
        "   * Calculate all the similarity scores against the query vector."
      ],
      "metadata": {
        "id": "lnHx9bfGcA_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute attention scores between \"journey\" and all other words\n",
        "attn_scores_1 = torch.matmul(keys, query_1)  # Dot product between query and all keys\n",
        "\n",
        "# Print the attention scores\n",
        "print(\"Attention Scores for 'journey' with all words:\")\n",
        "print(attn_scores_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMkqkPdgcLOE",
        "outputId": "54e6c33a-1e75-44fc-8baa-de7985c62d12"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores for 'journey' with all words:\n",
            "tensor([0.8114, 3.6527, 3.5677, 2.4092, 1.0304, 3.3444])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 2.2.5 Attention weights for inputs[1]:\n",
        "    * Normalize the attention scores."
      ],
      "metadata": {
        "id": "51GuGiHDcrRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Softmax to normalize scores into probabilities\n",
        "softmax = nn.Softmax(dim=0)\n",
        "attn_weights_1 = softmax(attn_scores_1)\n",
        "\n",
        "# Print the attention weights\n",
        "print(\"Attention Weights for 'journey':\")\n",
        "print(attn_weights_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8tvvD3YcwCl",
        "outputId": "df73baf0-c95c-405e-9194-1e0caeb05b93"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights for 'journey':\n",
            "tensor([0.0190, 0.3255, 0.2989, 0.0939, 0.0236, 0.2391])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 2.2.6 Calculate Context vector for inputs[1]:\n",
        "    * Generate the context vector."
      ],
      "metadata": {
        "id": "atpCwQaYdIL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the context vector as a weighted sum of value vectors\n",
        "context_vector_1 = torch.matmul(attn_weights_1, values)\n",
        "\n",
        "# Print the context vector for \"journey\"\n",
        "print(\"Context Vector for 'journey':\")\n",
        "print(context_vector_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIK1teM-dQjp",
        "outputId": "7833fe58-c284-4305-cdfb-0f0197451f96"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector for 'journey':\n",
            "tensor([ 0.3961, -0.5330,  1.4890])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  2.3 Computing Weight Parameters for All Inputs:\n",
        "  * 2.3.2 Compute the query, key, and value vectors:\n",
        "    * Compute the transformed vectors for all input words."
      ],
      "metadata": {
        "id": "Q34mMEsTdUJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the query, key, and value vectors for all input words\n",
        "queries = torch.matmul(inputs, Wq)  # Transform input into query space\n",
        "keys = torch.matmul(inputs, Wk)  # Transform input into key space\n",
        "values = torch.matmul(inputs, Wv)  # Transform input into value space\n",
        "\n",
        "# Print the transformed query, key, and value vectors\n",
        "print(\"Query Vectors (Q):\")\n",
        "print(queries)\n",
        "print(\"\\nKey Vectors (K):\")\n",
        "print(keys)\n",
        "print(\"\\nValue Vectors (V):\")\n",
        "print(values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J-t9mxJdb0U",
        "outputId": "9f1aebde-19f5-4cff-aa00-a12012ec2ac6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Vectors (Q):\n",
            "tensor([[ 2.1446, -0.6809,  0.4837],\n",
            "        [ 1.8430, -1.3271,  0.2715],\n",
            "        [ 1.8009, -1.2893,  0.2707],\n",
            "        [ 0.9364, -0.8335,  0.0959],\n",
            "        [ 0.5377, -0.2453,  0.1801],\n",
            "        [ 1.4156, -1.2427,  0.1166]])\n",
            "\n",
            "Key Vectors (K):\n",
            "tensor([[ 1.1341,  1.1532,  0.9270],\n",
            "        [ 1.7453, -0.3033,  0.1241],\n",
            "        [ 1.7092, -0.2853,  0.1437],\n",
            "        [ 1.0189, -0.4261, -0.1259],\n",
            "        [ 0.5792,  0.1216,  0.4577],\n",
            "        [ 1.4285, -0.5979, -0.3012]])\n",
            "\n",
            "Value Vectors (V):\n",
            "tensor([[ 0.0242, -0.3219,  1.1661],\n",
            "        [ 0.3617, -0.6609,  1.7805],\n",
            "        [ 0.3270, -0.6705,  1.7812],\n",
            "        [ 0.3225, -0.3367,  0.9311],\n",
            "        [-0.3900, -0.6543,  1.2925],\n",
            "        [ 0.6656, -0.2688,  0.9911]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 2.3.3 Compute the Attention Score for all inputs:\n",
        "   * Compute all attention scores between all words."
      ],
      "metadata": {
        "id": "k1FSfu57dwDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute attention scores for all words (Q * K^T)\n",
        "attn_scores_all = torch.matmul(queries, keys.T)  # Matrix multiplication\n",
        "\n",
        "# Print the full attention score matrix\n",
        "print(\"Attention Score Matrix for All Words:\")\n",
        "print(attn_scores_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLtdIyI3d3eN",
        "outputId": "6f87d38f-9f59-4d1e-def4-0e971ec91888"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Score Matrix for All Words:\n",
            "tensor([[2.0954, 4.0095, 3.9294, 2.4144, 1.3808, 3.3249],\n",
            "        [0.8114, 3.6527, 3.5677, 2.4092, 1.0304, 3.3444],\n",
            "        [0.8065, 3.5678, 3.4850, 2.3503, 1.0102, 3.2620],\n",
            "        [0.1896, 1.8989, 1.8520, 1.2972, 0.4849, 1.8071],\n",
            "        [0.4938, 1.0351, 1.0149, 0.6297, 0.3640, 0.8605],\n",
            "        [0.2803, 2.8620, 2.7909, 1.9572, 0.7222, 2.7301]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 2.3.4 Attention weights for all inputs:\n",
        "   * Normalize the attention scores."
      ],
      "metadata": {
        "id": "JwvqFhvMd-2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Softmax along each row (dim=1), so that each word's attention scores sum to 1\n",
        "softmax = nn.Softmax(dim=1)\n",
        "attn_weights_all = softmax(attn_scores_all)\n",
        "\n",
        "# Print the full attention weight matrix\n",
        "print(\"Attention Weights Matrix for All Words:\")\n",
        "print(attn_weights_all)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gptcBznLeFk2",
        "outputId": "4583ce05-dbcc-400b-ec10-dfab15bf1b56"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights Matrix for All Words:\n",
            "tensor([[0.0518, 0.3509, 0.3239, 0.0712, 0.0253, 0.1770],\n",
            "        [0.0190, 0.3255, 0.2989, 0.0939, 0.0236, 0.2391],\n",
            "        [0.0204, 0.3232, 0.2975, 0.0957, 0.0250, 0.2381],\n",
            "        [0.0472, 0.2605, 0.2486, 0.1427, 0.0633, 0.2377],\n",
            "        [0.1271, 0.2184, 0.2140, 0.1456, 0.1116, 0.1834],\n",
            "        [0.0222, 0.2936, 0.2735, 0.1188, 0.0346, 0.2573]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * 2.3.5 Calculate Context vector for all inputs:\n",
        "   * Generate all context vectors."
      ],
      "metadata": {
        "id": "OCoN326SeKIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the context vectors for all words\n",
        "context_vectors_all = torch.matmul(attn_weights_all, values)\n",
        "\n",
        "# Print the full context vector matrix\n",
        "print(\"Context Vectors for All Words:\")\n",
        "print(context_vectors_all)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veFd7ihleQso",
        "outputId": "192da8e6-614c-4929-97bd-1bb4ac996520"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vectors for All Words:\n",
            "tensor([[ 0.3649, -0.5539,  1.5364],\n",
            "        [ 0.3961, -0.5330,  1.4890],\n",
            "        [ 0.3943, -0.5323,  1.4867],\n",
            "        [ 0.3562, -0.5074,  1.4120],\n",
            "        [ 0.2775, -0.5001,  1.3797],\n",
            "        [ 0.3923, -0.5164,  1.4461]])\n"
          ]
        }
      ]
    }
  ]
}