{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to Finetune LLMs with LoRA\n",
        "\n",
        "\n",
        "Parameter-Efficient Fine-Tuning (PEFT) methods, like LoRA, address the challenges of fine-tuning large language models (LLMs) by only updating a small subset of the model’s parameters. This approach significantly reduces computational and storage costs, making LLM fine-tuning more accessible.\n",
        "\n",
        "PEFT techniques allow developers to adapt pre-trained models to specific tasks without retraining the entire model, leading to faster development cycles and reduced resource consumption.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eh-0j-KFqXmw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzOxcgn9qJ8p",
        "outputId": "ca7664c0-9073-4ca2-e86c-f447b26497cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/72.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/487.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install peft==0.4.0 datasets accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # File and directory management (e.g., saving/loading models)\n",
        "import torch  # Deep learning framework used for model training and inference\n",
        "import time  # Used for measuring execution time (benchmarking)\n",
        "\n",
        "# Import Hugging Face Transformers for handling pre-trained language models\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Import dataset loading utility from Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import PEFT (Parameter-Efficient Fine-Tuning) modules for LoRA-based training\n",
        "from peft import LoraConfig  # Defines LoRA configurations (rank, dropout, etc.)\n",
        "from peft import get_peft_model  # Applies LoRA to a pre-trained model\n",
        "from peft import PeftModel  # Loads a fine-tuned LoRA model for inference"
      ],
      "metadata": {
        "id": "L5a6Ara0IR12"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a Cache Directory"
      ],
      "metadata": {
        "id": "IzkVqBFAJJ7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the \"cache\" directory exists to store temporary files or model checkpoints\n",
        "if not os.path.exists(\"cache\"):\n",
        "    os.makedirs(\"cache\")  # Create the directory if it doesn't exist\n",
        "    print(\" 'cache' directory created!\")\n",
        "else:\n",
        "    print(\" 'cache' directory already exists.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb7NVOdX1EZw",
        "outputId": "314bb5c6-12de-45fa-a070-bfb51e8cfb4b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 'cache' directory already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the Pre-trained Model and Tokenizer\n",
        "This cell initializes the tokenizer and pre-trained language model, which will later be fine-tuned using LoRA.\n"
      ],
      "metadata": {
        "id": "oNa1nLW2KTFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model name (BigScience's BLOOMZ-560M)\n",
        "model_name = \"bigscience/bloomz-560m\"\n",
        "\n",
        "# Load pre-trained tokenizer for BLOOMZ\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load pre-trained causal language model (optimized for hardware)\n",
        "foundation_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,  # Use efficient float16 on GPU if available\n",
        "    device_map=\"auto\"  # Automatically assign model to GPU or CPU\n",
        ")\n",
        "\n",
        "# Confirm successful loading\n",
        "print(\" Model and tokenizer loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnF7JfeGq4GM",
        "outputId": "7a93ee6e-1c7b-4ea4-c8b4-9cbd8fa9b7d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model and tokenizer loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading and Preprocessing the Dataset\n",
        "This cell loads a dataset of English quotes, preprocesses it, and prepares it for fine-tuning.\n"
      ],
      "metadata": {
        "id": "XBEI7LE3KvpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load only 10% of the dataset for efficient training\n",
        "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\").shuffle(seed=42).select(range(int(0.1 * len(load_dataset(\"Abirate/english_quotes\", split=\"train\")))))  # Select 10% of data\n",
        "\n",
        "# Tokenize the dataset (convert quotes to token IDs)\n",
        "data = dataset.map(lambda samples: tokenizer(samples[\"quote\"],\n",
        "                                             padding=\"max_length\",  # Pad all sequences to the same length\n",
        "                                             truncation=True,  # Truncate if longer than max_length\n",
        "                                             max_length=128  # Define max token length\n",
        "                                             ),\n",
        "                   batched=True)  # Apply tokenization in batches for efficiency\n",
        "\n",
        "# Select a small sample (10 samples) for inspection\n",
        "train_sample = data.select(range(10))\n",
        "\n",
        "# Print confirmation message\n",
        "print(f\"Dataset loaded and tokenized! Number of samples: {len(data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbkjp0DFrG1c",
        "outputId": "0a6cc3f5-2044-4a4a-e7cf-f5acab890345"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded and tokenized! Number of samples: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5htN-7g8oXr",
        "outputId": "1e440843-0701-4681-b347-1cf27ce3f5d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 250\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configuring LoRA for Efficient Fine-Tuning\n",
        "Now, we define the LoRA (Low-Rank Adaptation) configuration, which enables efficient fine-tuning by modifying only a subset of the model's parameters instead of the entire network. This significantly reduces computational costs while maintaining strong performance."
      ],
      "metadata": {
        "id": "92C4b31WLsOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LoRA configuration for parameter-efficient fine-tuning\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # LoRA rank: Controls parameter reduction (smaller rank = more efficiency)\n",
        "    lora_alpha=32,  # Scaling factor: Adjusts the impact of LoRA weight updates\n",
        "    target_modules=[\"query_key_value\"],  # Apply LoRA only to attention layers (query-key-value projection)\n",
        "    lora_dropout=0.1,  # Regularization: Dropout to prevent overfitting\n",
        "    bias=\"none\",  # No additional bias training\n",
        "    task_type=\"CAUSAL_LM\"  # Fine-tuning for autoregressive text generation\n",
        ")\n",
        "\n",
        "print(\" LoRA configuration set successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfbrhuYvrRsW",
        "outputId": "f5343877-0dd9-482c-cf32-132d145a9c57"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LoRA configuration set successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Applying LoRA to the Pre-trained Model\n",
        "\n",
        "Next, we integrate the LoRA (Low-Rank Adaptation) layers into the pre-trained model to enable efficient fine-tuning. Instead of updating all model parameters, we introduce trainable LoRA layers while keeping the majority of the original model frozen."
      ],
      "metadata": {
        "id": "pIoLRtb4ME6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA to the pre-trained model\n",
        "lora_model = get_peft_model(foundation_model, lora_config)\n",
        "\n",
        "# Compute the number of trainable and frozen parameters\n",
        "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
        "frozen_params = sum(p.numel() for p in lora_model.parameters() if not p.requires_grad)\n",
        "\n",
        "# Display the parameter details\n",
        "print(f\"  LoRA model initialized successfully!\")\n",
        "print(f\" - Trainable parameters: {trainable_params:,}\")\n",
        "print(f\" - Frozen parameters: {frozen_params:,}\")\n",
        "print(f\" - Percentage of trainable parameters: {100 * trainable_params / (trainable_params + frozen_params):.4f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZTSLxTorlmf",
        "outputId": "790ea50f-4a87-434a-ab08-4bb26ec5f24b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  LoRA model initialized successfully!\n",
            " - Trainable parameters: 786,432\n",
            " - Frozen parameters: 559,214,592\n",
            " - Percentage of trainable parameters: 0.1404%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configuring Training Arguments\n",
        "we define training parameters using the TrainingArguments class from Hugging Face's transformers library. These arguments control how the fine-tuning process is executed.\n",
        "\n"
      ],
      "metadata": {
        "id": "UqNKyrt_MzyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define outputs directory\n",
        "output_directory = os.path.join(\"./cache\", \"peft_lab_outputs\")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    report_to=\"none\",                # Disable reporting to external services like WandB\n",
        "    output_dir=output_directory,     # Directory to save model checkpoints\n",
        "    auto_find_batch_size=True,       # Automatically find batch size\n",
        "    # evaluation_strategy=\"epoch\",     # Evaluate model at the end of each epoch\n",
        "    save_strategy=\"epoch\",           # Save model at the end of each epoch\n",
        "    learning_rate=3e-4,              # LoRA often requires a higher learning rate\n",
        "    # per_device_train_batch_size=8,   # Number of samples per batch (adjust based on GPU memory)\n",
        "    # per_device_eval_batch_size=8,    # Batch size for evaluation\n",
        "    # save_total_limit=2,              # Keep only the last 2 checkpoints\n",
        "    #  weight_decay=0.01,               # Regularization to prevent overfitting\n",
        "    # logging_dir=\"./logs\",            # Where logs are stored\n",
        "    # logging_steps=10,                # Log metrics every 10 steps\n",
        "    num_train_epochs=5              # Number of epochs (adjust based on dataset size)\n",
        "\n",
        ")\n",
        "\n",
        "print(\" Training arguments set up successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmLiAY8ar50t",
        "outputId": "d13a3946-83d7-4e92-b3c0-29ac632ec58a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training arguments set up successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initializing and Running the Trainer\n",
        "We initialize the Trainer using Hugging Face's Trainer class. This step trains the LoRA-enhanced model using the preprocessed dataset."
      ],
      "metadata": {
        "id": "O3LWSySKNXeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=lora_model,  # LoRA-enhanced model to fine-tune\n",
        "    args=training_args,  # Training configurations (learning rate, epochs, etc.)\n",
        "    train_dataset=data,  # Tokenized training dataset\n",
        "    # eval_dataset=data,   # Evaluation dataset (optional, same as train dataset)\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)  # 🛠️ Handles padding\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "ktG5rcK8CCU7",
        "outputId": "b5bfff45-b906-4469-e472-950641f372c7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [160/160 00:13, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=160, training_loss=3.273973846435547, metrics={'train_runtime': 13.9262, 'train_samples_per_second': 89.759, 'train_steps_per_second': 11.489, 'total_flos': 290975907840000.0, 'train_loss': 3.273973846435547, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving the Fine-Tuned LoRA Model\n",
        "Now that training is complete, we save the fine-tuned LoRA model so that it can be reloaded for inference later."
      ],
      "metadata": {
        "id": "LELxwittN5Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory to save the fine-tuned model\n",
        "time_now = time.strftime(\"%Y-%m-%d_%H-%M-%S\")  # Generate timestamp\n",
        "peft_model_path = os.path.join(output_directory, f\"peft_model_{time_now}\")  # Create unique model path\n",
        "\n",
        "# Save the fine-tuned LoRA model\n",
        "trainer.model.save_pretrained(peft_model_path)\n",
        "\n",
        "# Confirm successful save\n",
        "print(f\" LoRA fine-tuned model saved successfully at: {peft_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzHdFPcvFLhW",
        "outputId": "69a0fe8c-dc67-437d-a547-9e44be59608e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LoRA fine-tuned model saved successfully at: ./cache/peft_lab_outputs/peft_model_2025-03-19_07-05-00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the Fine-Tuned LoRA Model for Inference\n",
        "After that we have saved the fine-tuned LoRA model, we need to load it back for inference (text generation).\n",
        "\n"
      ],
      "metadata": {
        "id": "BEmLCy8rOXvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the base model (bloomz-560m)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)  # Load pre-trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load tokenizer\n",
        "\n",
        "# Load the fine-tuned LoRA model\n",
        "peft_model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
        "\n",
        "# Confirmation message\n",
        "print(\" LoRA fine-tuned model loaded successfully for inference!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jR20m4ZFZvj",
        "outputId": "53cbadb5-1cdd-4d07-e6a4-cf7e48fa93e9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LoRA fine-tuned model loaded successfully for inference!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating Text with the Fine-Tuned LoRA Model\n",
        "\n",
        "Finally, we will use loaded fine_tuned LoRA model to generate text based on a given prompt."
      ],
      "metadata": {
        "id": "l2FkVeSjFmA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display 5 random quotes from the dataset\n",
        "import random\n",
        "\n",
        "# Select 5 random samples\n",
        "sample_quotes = data.select(random.sample(range(len(data)), 5))\n",
        "\n",
        "# Print the quotes\n",
        "for i, sample in enumerate(sample_quotes[\"quote\"]):\n",
        "    print(f\"{i+1}. {sample}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgMmPRc1QDw_",
        "outputId": "ba178432-8416-4a6f-c76a-18874c35a0c1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. “Don't spend time beating on a wall, hoping to transform it into a door. ”\n",
            "2. “The homemaker has the ultimate career. All other careers exist for one purpose only - and that is to support the ultimate career. ”\n",
            "3. “One, remember to look up at the stars and not down at your feet. Two, never give up work. Work gives you meaning and purpose and life is empty without it. Three, if you are lucky enough to find love, remember it is there and don't throw it away.”\n",
            "4. “If you want to keep a secret, you must also hide it from yourself.”\n",
            "5. “I will not let anyone walk through my mind with their dirty feet.”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input prompt\n",
        "input_text = \"Don't spend time beating on a wall,\"\n",
        "\n",
        "# Tokenize input text (convert to numerical format)\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text using the fine-tuned LoRA model\n",
        "outputs = peft_model.generate(\n",
        "    input_ids=inputs[\"input_ids\"],  # Tokenized input prompt\n",
        "    attention_mask=inputs[\"attention_mask\"],  # Mask to focus on real input\n",
        "    max_length=100,  # Limit output length\n",
        "    num_return_sequences=1,  # Generate 1 sequence\n",
        "    do_sample=True,  # Enable randomness for diverse responses\n",
        "    top_k=50,  # Consider top 50 tokens for sampling\n",
        "    top_p=0.95  # Nucleus sampling: keep top 95% probability mass\n",
        ")\n",
        "\n",
        "# Decode generated tokens back into readable text\n",
        "print(\"\\n Generated Text:\")\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))  # Remove special tokens like <eos>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78jLL1ceFm03",
        "outputId": "4df210e3-7d05-4cc7-8b9d-9e73c7b9af03"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generated Text:\n",
            "[\"Don't spend time beating on a wall, talking to a man, or even listening to music on the cellar. Don’t waste time going through the woods by walking at a slow pace until you fall over.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "In this exercise, we successfully applied LoRA fine-tuning to a pre-trained language model (bloomz-560m) using a dataset of English quotes. Our goal was to see how well the fine-tuned model could generate meaningful text completions.\n",
        "\n",
        "Key Observations:\n",
        "* LoRA efficiently fine-tuned the model with minimal computational cost.\n",
        "* The model learned general sentence patterns from the dataset.\n",
        "* However, it struggled to accurately complete quotes, often generating text that was coherent but not contextually relevant.\n",
        "* The limited training data (10% sample) likely reduced the model’s ability to specialize in quote completions.\n",
        "\n",
        "Takeaways:\n",
        "\n",
        "LoRA is a lightweight yet powerful fine-tuning method.\n",
        "* The choice of training data size and quality significantly impacts model performance.\n",
        "* Further improvements could be made by refining the dataset, adjusting hyperparameters, and using a more robust base model.\n",
        "* Overall, this challenge provided a hands-on understanding of how LoRA fine-tuning works in practice.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5RVDdZWpRhEv"
      }
    }
  ]
}