{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GQkWBIployl"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVmNY4B-D01L",
    "outputId": "35f29b12-3283-4103-dc03-6eee21731f60"
   },
   "outputs": [],
   "source": [
    "# On initialise une mémoire conversationnelle de type \"buffer\"\n",
    "# Elle va stocker les échanges successifs entre l'utilisateur et le chatbot.\n",
    "\n",
    "memory = ConversationBufferMemory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCzI0pk7EBXu"
   },
   "source": [
    "ConversationBufferMemory() est l’objet qui :\n",
    "\n",
    "garde en mémoire les échanges passés,\n",
    "\n",
    "les stocke dans une variable interne appelée chat_memory,\n",
    "\n",
    "est capable de restituer toute la conversation au fur et à mesure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QozQYQfbEC7c"
   },
   "source": [
    "#### Simulons un premier échange utilisateur-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GX5kRcl7EKLf"
   },
   "outputs": [],
   "source": [
    "# Premier message simulé de l'utilisateur\n",
    "user_input_1 = \"Hello, how are you?\"\n",
    "\n",
    "# Première réponse simulée du bot\n",
    "bot_output_1 = \"I'm fine, thank you. How can I assist you today?\"\n",
    "\n",
    "# On enregistre ce premier échange dans la mémoire\n",
    "memory.save_context(\n",
    "    {\"input\": user_input_1},   # dictionnaire d'entrée\n",
    "    {\"output\": bot_output_1}   # dictionnaire de sortie\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXy9Byb9EYw-"
   },
   "source": [
    "save_context() est la méthode principale pour enregistrer un échange :\n",
    "\n",
    "Elle prend un input (ce que dit l'utilisateur),\n",
    "\n",
    "et un output (ce que dit le bot),\n",
    "\n",
    "puis l’ajoute dans le \"buffer\" de mémoire interne.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxIDmOU1EtoA"
   },
   "source": [
    "#### Simulons un deuxième échange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wg6kguewEy9R"
   },
   "outputs": [],
   "source": [
    "# Deuxième message de l'utilisateur\n",
    "user_input_2 = \"Tell me a joke.\"\n",
    "\n",
    "# Deuxième réponse du bot\n",
    "bot_output_2 = \"Why did the chicken cross the road? To get to the other side.\"\n",
    "\n",
    "# On enregistre ce deuxième échange dans la mémoire\n",
    "memory.save_context(\n",
    "    {\"input\": user_input_2},\n",
    "    {\"output\": bot_output_2}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBa3KWHRE-dY"
   },
   "source": [
    "Le même principe : on enrichit la mémoire avec chaque nouveau tour de conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3mOkWSzFE3k"
   },
   "source": [
    "Récupérons l’historique complet de la conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wR-Sbg60FJz2",
    "outputId": "11609db1-52d9-4785-87df-1ea5263e094a"
   },
   "outputs": [],
   "source": [
    "# On récupère l'historique entier des échanges utilisateur ↔ bot\n",
    "conversation_history = memory.load_memory_variables({})\n",
    "\n",
    "# On affiche le résultat\n",
    "print(conversation_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QljywS_0FTuE"
   },
   "source": [
    "load_memory_variables({}) renvoie un dictionnaire contenant :\n",
    "\n",
    "Une clé \"history\"\n",
    "\n",
    "Une valeur : le texte complet des échanges (user + bot)\n",
    "\n",
    "C’est ce que le LLM peut utiliser pour garder du contexte dans une vraie application."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
