{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwgFgw2oQP53"
   },
   "source": [
    "Sentiment Analysis of Amazon Reviews and Automated Response Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWbSVmxg9RXs"
   },
   "source": [
    "# 2. LLM affinÃ© pour l'analyse des sentiments et les rÃ©ponses contextuelles\n",
    "\n",
    "\n",
    "#### **AperÃ§u**\n",
    "EntraÃ®nez un LLM optimisÃ© en termes de paramÃ¨tres (par exemple, en utilisant LoRA ) pour classer les sentiments et gÃ©nÃ©rer des rÃ©ponses contextuelles .\n",
    "\n",
    "\n",
    "\n",
    "#### **Pourquoi Ã§a convient ?**\n",
    "Couvre le rÃ©glage fin, l'efficacitÃ© des paramÃ¨tres (LoRA), l'architecture du transformateur, les intÃ©grations, RAG, les techniques d'Ã©valuation .\n",
    "\n",
    "\n",
    "\n",
    "#### **Composants clÃ©s**\n",
    "* SÃ©lectionnez un LLM prÃ©-entraÃ®nÃ© (par exemple, distilBERT) et affinez-le pour la classification des sentiments (avis IMDB, donnÃ©es Twitter, etc.).\n",
    "* ImplÃ©mentez LoRA pour un rÃ©glage fin efficace des paramÃ¨tres sur le processeur .\n",
    "Utilisez des techniques de rÃ©cupÃ©ration (FAISS/Pinecone) pour des rÃ©ponses contextuelles.\n",
    "* DÃ©ployez une interface utilisateur Streamlit pour les interactions basÃ©es sur les sentiments.\n",
    "* Ã‰valuez les performances du modÃ¨le avec prÃ©cision, perplexitÃ©, BLEU et ROUGE .\n",
    "Bonus : IntÃ©grez les flux de travail dâ€™Ã©valuation LLM aux mÃ©canismes de rÃ©troaction humaine .\n",
    "\n",
    "\n",
    "#### **ConsidÃ©rations favorables au processeur**\n",
    "*  **ModÃ¨les recommandÃ©s** : *distilBERT , BERT-base-uncased, roberta-small,albert-base-v2*\n",
    "*  **Utilisation de la bibliothÃ¨que** : *transformers , optimum, peft,sentence-transformers*\n",
    "*  **Conseils d'optimisation** :\n",
    "  * Utiliser LoRA (Low-Rank Adaptation) au lieu d'un rÃ©glage fin complet\n",
    "  * RÃ©duire la taille de l'ensemble de donnÃ©es (par exemple, prendre seulement 10 000 Ã©chantillons de l'ensemble de donnÃ©es IMDB)\n",
    "  * Utilisez des modÃ¨les de transformateurs plus petits ( distilBERTplus de BERT-base, albert-baseplus de BERT-large)\n",
    "  * Utilisez la prÃ©cision mixte ( fp16) si elle est prise en charge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hfv_0KzFh9Mp",
    "outputId": "a9ddcfd2-027f-4a88-a484-bc0e94c81cb7"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "\n",
    "# Install core packages for Hugging Face models and training\n",
    "command = [\n",
    "    \"pip\", \"install\", \"--upgrade\",\n",
    "    \"datasets\", \"transformers\", \"peft\", \"accelerate\", \"bitsandbytes\"\n",
    "]\n",
    "\n",
    "# Run the pip command silently\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Show only the last 5 lines of the output\n",
    "lines = result.stdout.strip().split(\"\\n\")\n",
    "print(\"Last installation lines:\\n\")\n",
    "print(\"\\n\".join(lines[-5:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edyl2HUSRBvs",
    "outputId": "86f80abf-cb8f-4a25-fff2-2206302b6ef9"
   },
   "outputs": [],
   "source": [
    "# Clean Torch installation for GPU compatibility\n",
    "print(\" - Uninstalling any existing torch packages...\")\n",
    "subprocess.run([\"pip\", \"uninstall\", \"-y\", \"torch\", \"torchvision\", \"torchaudio\"],\n",
    "               capture_output=True, text=True)\n",
    "\n",
    "# Install compatible GPU version (Colab auto-detects the right one)\n",
    "print(\" - Installing torch (GPU version if available)...\")\n",
    "install_cmd = [\"pip\", \"install\", \"--upgrade\", \"torch\", \"torchvision\", \"torchaudio\"]\n",
    "result = subprocess.run(install_cmd, capture_output=True, text=True)\n",
    "\n",
    "print(\"\\n - Last lines from torch installation:\")\n",
    "print(\"\\n\".join(result.stdout.strip().split(\"\\n\")[-5:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VD3OO653Mq6E",
    "outputId": "244e57cc-e877-4019-8bd7-5d2c19d161ae"
   },
   "outputs": [],
   "source": [
    "# Check torch version and path\n",
    "import torch\n",
    "print(torch.__file__)\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Torch is working \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8DMM_FjyTASx"
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Essential Libraries for BERT Sentiment Classification\n",
    "# ============================\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Class balancing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Deep learning with PyTorch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Hugging Face Transformers\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertTokenizerFast,\n",
    "    BertConfig,\n",
    "    BertForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Hugging Face Datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "# Hugging Face Hub login\n",
    "from huggingface_hub import login\n",
    "from getpass import getpass\n",
    "\n",
    "# PEFT (LoRA fine-tuning)\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "Pk-BzB12U2dd",
    "outputId": "9da7abf8-876a-469c-b69a-48a18682ac05"
   },
   "outputs": [],
   "source": [
    "# Upload your kaggle.json file\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awVq8_-AVIXB",
    "outputId": "a6417a5c-9a38-4b87-a2bf-9b8d4788ccbb"
   },
   "outputs": [],
   "source": [
    "# Move kaggle API key to the correct location\n",
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json  # Secure file permissions\n",
    "\n",
    "# Download and unzip the dataset\n",
    "!kaggle datasets download -d halimedogan/amazon-reviews -p /content --unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wvv_4fhqVx6-",
    "outputId": "407e4c39-da09-4a12-801c-ea9a2ea96957"
   },
   "outputs": [],
   "source": [
    "# Check download success\n",
    "print(os.listdir(\"/content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tocu7XeNV3b3",
    "outputId": "24150657-5a67-460c-b854-c82a8cbeb777"
   },
   "outputs": [],
   "source": [
    "# Load dataset using pandas\n",
    "data = pd.read_csv(\"/content/amazon_reviews.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtpwlZmUDWLr"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "a6A6QLtBQP57",
    "outputId": "e000ab0c-0dd8-4349-f8a8-877f7a511892"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z9xjEOxKQP57",
    "outputId": "939a45b8-cb92-49b0-ccca-b3bef24428cd"
   },
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HHecqb7xUPus",
    "outputId": "14083be0-8dc1-4009-82fe-e1e049a58ad6"
   },
   "outputs": [],
   "source": [
    "data[\"reviewerName\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "JZUekucyV9WK",
    "outputId": "a7fdc75e-c058-4933-91f8-74ff061bf248"
   },
   "outputs": [],
   "source": [
    "# Quick peek at the data (examples)\n",
    "data[\"reviewText\"][0]  # â†’ \"No issues\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "igUhSoSzWJQY",
    "outputId": "455727df-38a4-437c-dbae-e4d4a879941e"
   },
   "outputs": [],
   "source": [
    "data[\"reviewText\"][1]  # â†’ \"Purchased this for my device, it worked as advertised...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdAAOKJpXmKH",
    "outputId": "cb683fa0-62db-4974-991d-dab85cdb5d9a"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in reviewText:\", data['reviewText'].isna().sum())\n",
    "print(\"Missing values in summary    :\", data['summary'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnqdR0k6XvKz"
   },
   "outputs": [],
   "source": [
    "# Drop row with missing reviewText\n",
    "data = data.dropna(subset=['reviewText']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9vl_B6RQfsI"
   },
   "source": [
    " ### Light Text Cleaning (for BERT compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yudzdf1-cuEl"
   },
   "outputs": [],
   "source": [
    "# Define a simple cleaning function suitable for BERT-like models\n",
    "def light_clean(text):\n",
    "    \"\"\"\n",
    "    Light text cleaning:\n",
    "    - Lowercasing\n",
    "    - Removing URLs\n",
    "    - Trimming extra spaces\n",
    "    This kind of preprocessing is aligned with the expectations of pre-trained models like BERT, DistilBERT, etc.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = str(text).lower()                     # Lowercasing\n",
    "    text = re.sub(r'http\\S+', '', text)          # Remove URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()     # Normalize whitespace\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fDd2HNkTjH7"
   },
   "outputs": [],
   "source": [
    "# Apply cleaning to 'reviewText' and 'summary'\n",
    "data_cleaned = data.copy()\n",
    "\n",
    "for col in ['reviewText', 'summary']:\n",
    "    data_cleaned[f\"clean_{col}\"] = data_cleaned[col].fillna('').apply(light_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtaaLKvxc6M-"
   },
   "outputs": [],
   "source": [
    "# Combine summary and full text into one field for classification\n",
    "data_cleaned[\"clean_text\"] = (\n",
    "    data_cleaned['clean_summary'] + \" \" + data_cleaned['clean_reviewText']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exW3etpjIeN9"
   },
   "source": [
    "### ðŸ“Š EDA â€“ Target Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVQjyy1oQP58",
    "outputId": "1ac01057-3068-482c-b548-c149116856bf"
   },
   "outputs": [],
   "source": [
    "# Count number of reviews per 'overall' score\n",
    "overall_counts = data_cleaned.groupby(\"overall\")[\"reviewerID\"].count().reset_index()\n",
    "print(overall_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "3HmNkE1KQP59",
    "outputId": "b6a177f9-c062-4f3a-e1c6-20e7dce74cf6"
   },
   "outputs": [],
   "source": [
    "# Barplot of score distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "counts = data_cleaned[\"overall\"].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "ax = sns.barplot(x=counts.index, y=counts.values, palette=\"viridis\")\n",
    "\n",
    "for i, v in enumerate(counts.values):\n",
    "    ax.text(i, v + 10, str(v), ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.xlabel(\"Overall Rating\", fontsize=12)\n",
    "plt.ylabel(\"Number of Reviews\", fontsize=12)\n",
    "plt.title(\"Distribution of Ratings\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz9ETJJDIywN"
   },
   "source": [
    "### ðŸŽ¯ Sentiment Classification Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tyhlSwmemzE",
    "outputId": "f1bc377b-2324-4739-cde9-d006f17a83d1"
   },
   "outputs": [],
   "source": [
    "# Remove neutral reviews (score = 3)\n",
    "data_cleaned = data_cleaned[data_cleaned['overall'] != 3]\n",
    "\n",
    "# Create binary sentiment label\n",
    "data_cleaned['label'] = data_cleaned['overall'].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "a-oFHjs5QP5-",
    "outputId": "2926f144-c125-4857-fd73-3908b4277a63"
   },
   "outputs": [],
   "source": [
    "# Visualize class distribution (positive vs negative)\n",
    "plt.figure(figsize=(6,4))\n",
    "ax = sns.countplot(x=data_cleaned[\"label\"], palette=[\"red\", \"green\"])\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height()}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='bottom', fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "plt.xticks([0, 1], [\"Negative\", \"Positive\"])\n",
    "plt.xlabel(\"Sentiment\", fontsize=12)\n",
    "plt.ylabel(\"Number of Reviews\", fontsize=12)\n",
    "plt.title(\"Sentiment Distribution\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "Tk_PG7X0QP5-",
    "outputId": "e9027027-64de-4b9e-f740-3b3aee346a57"
   },
   "outputs": [],
   "source": [
    "# Final dataset with clean text and label\n",
    "df = data_cleaned[[\"reviewerID\", \"clean_text\", \"label\"]].copy()\n",
    "df[\"sentiment\"] = df[\"label\"].apply(lambda x: \"positive\" if x == 1 else \"negative\")\n",
    "\n",
    "print(\"Final DataFrame shape (reviewerID, clean_text, label):\", df.shape)\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gswnGFG2YAUy"
   },
   "source": [
    "### ðŸ“ Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ve0FHQohX_Ly",
    "outputId": "6a231378-22a7-4e45-d5db-d9cd0d1d9526"
   },
   "outputs": [],
   "source": [
    "# Add word count per review\n",
    "data_cleaned[\"text_length\"] = data_cleaned[\"clean_text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Summary statistics\n",
    "print(\" > Text length statistics:\\n\")\n",
    "print(\" > Mean     :\", data_cleaned[\"text_length\"].mean())\n",
    "print(\" > Median   :\", data_cleaned[\"text_length\"].median())\n",
    "print(\" > Min      :\", data_cleaned[\"text_length\"].min())\n",
    "print(\" > Max      :\", data_cleaned[\"text_length\"].max())\n",
    "print(\" > Std Dev  :\", data_cleaned[\"text_length\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "po6LfB9nX-0m",
    "outputId": "88a810a3-fcca-461b-c44a-1a19ee0e5c73"
   },
   "outputs": [],
   "source": [
    "# Histogram of text lengths\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(data_cleaned[\"text_length\"], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Text Length Distribution\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0A_H6uMhAmc"
   },
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fqaZC_XhNNh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data while keeping class distribution consistent\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['clean_text'],\n",
    "    df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vqj0ExtmYYLP",
    "outputId": "68c7716d-1957-4e3d-f9d8-e0473ea39063"
   },
   "outputs": [],
   "source": [
    "# Wrap splits into dataframes\n",
    "train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})\n",
    "test_df = pd.DataFrame({'text': test_texts, 'label': test_labels})\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhivaDHrc0o_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTkcxsD1OnoV"
   },
   "source": [
    "### Class Weights & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "clw6PZpJaTVC",
    "outputId": "19ac22ba-23bb-4dbe-96fa-69666df18969"
   },
   "outputs": [],
   "source": [
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1]),\n",
    "    y=df['label']\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensor for use in the loss function\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "print(\"Class weights:\", class_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqdQn8sKhocm"
   },
   "source": [
    "### Convert to Hugging Face Dataset Format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOofCYG4hoG_"
   },
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Dataset objects\n",
    "# Required for using the Trainer API\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wU4ZWG4dhwu6"
   },
   "source": [
    "### Hugging Face Authentication (for model upload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvIPjL7FvU_s",
    "outputId": "b83291fd-d286-4a2b-a68b-186071bd011d"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Secure login with personal token\n",
    "huggingface_token = getpass(\"Enter your Hugging Face token: \")\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = huggingface_token\n",
    "login(token=huggingface_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JI1TTU-LPKVZ"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333,
     "referenced_widgets": [
      "a909fbed4dc346278fadd8a420213f9b",
      "6e50fa6f9f844d1db33425b94ca8bb0c",
      "bb19975db0764ceeb560ba30983ee579",
      "437d9545b7e7416b829da4277433fc55",
      "01d5b4b5660b41ff88a4d513bdb23e06",
      "c9827c0a248047d282ad7c5d880205e5",
      "ebf0a9cf97c047f39f2ddc75b64822fc",
      "8995487c3f8e455d9580f6275e89ee83",
      "7f31c15326bd412e99507ce1d607d498",
      "977de7b5de6242c89d397c485f77d243",
      "a6ddd13437404083b596a54c645614ed",
      "f86f2bd98a0c40aa81c227f81bf26f93",
      "9b9d8a52af624f67bdea36aab25bc33c",
      "1882144119e547d694b38589b4c94355",
      "61097473bf044b6a919b0608703679fe",
      "1865fbe0cef94ac48016aab55c831628",
      "3889a0414b5941b6bd80d760a11e83a6",
      "a8963487c8974ec3b4d28d728f8c9b97",
      "23e2a8ad82a3481f987a51d4004ef061",
      "c07ffb798c244341a8830704d4f8e7f4",
      "996858cd1f5a46e69545856093a8d235",
      "7b8a1757b6c041dbad69dee966095351",
      "637f545a1ebb4ca2bf74a07bbe8cbdf0",
      "970a7f209f834c67ab729ec4ee828c42",
      "7a35fee76c6645159426e641aaca70ff",
      "25d3ba96926a40e1a6b77486449e3e0f",
      "0bb601897df54ab2ac95fcedc5ad8979",
      "00d8d6b5e530476d87b3071a75918a97",
      "ccbcc98f53324903b2d29b0d034e97ec",
      "e05477d15b384d4baabc4f3436c3bdfd",
      "5d7db560596a44e5a7eaca0cf6bcf97b",
      "35b36ba1f840407b8eb664aa662c176d",
      "a4d1f08195c4406c9b964524f84c2be2",
      "7873ffbd40e146049b00df7efedcb9db",
      "4329a00ad9ae4e6e8b9f6b8ac3a63f8f",
      "6220e46a4d0f4ce7a2901f70e42d6620",
      "9f71a31ce3564014b7aaaf5ee669ca49",
      "e3de2440833b4cda9829eb4fd187a2a9",
      "7161ea690a714375b7170b25bae84fcc",
      "ad71ec6dc9cb4670bff5aa9183d1ecca",
      "83a42cbf067146e08d37c6a7850a22be",
      "498aa4e63cce448e8ef1a77c54c094da",
      "fdbf078181e34520a78fef5ee594df08",
      "fcef9496032746dd9176260cb416093f",
      "fab26a8a190d4d4e993ddf86cf3ba8bd",
      "fa136cc62a384407a995e0d646f81900",
      "4b904444fa2443ad9d3d91223e1d4249",
      "27f05cadca7f43748dc08c4c98dd9407",
      "4dfb1174132548c1bf0468c12eef2adb",
      "9e8477da94a549c9a6194340bb911546",
      "804076ab1e454d449767aa1ff029c2b9",
      "3dfc9e82ba4e45a796007d99533908f1",
      "c32cd916cd0e489f9836ac2bf87bbed4",
      "6ff8d28a06a243edb8561f1348b6890c",
      "cf990859b75c4a14873dee510d5b545b",
      "a84dc796c5ae4d9aba2b7557ba2a3245",
      "f7dff2fe31cd420d8f6a72942b836504",
      "098ffa52d8694b1fad4d2cf542eaa5b4",
      "9117185040554a3ca1d41291534a5093",
      "55aeeee7592b45c29f9fb462991acbc8",
      "13d2274ff3714d4dbd36491b01a60c93",
      "32cb78796dd3402d94f6c002e1c4bc85",
      "53b198c6c0424bfb9c542f2e975e1949",
      "7ba81943d12249bebbb157c0b9db3e4e",
      "e812e162382d4e89be4a85848d3141b8",
      "2035cf7760574eaa92e7f973319c3f1b"
     ]
    },
    "id": "tOyjltFmjXQD",
    "outputId": "463562bc-b6a5-4ed0-bb1b-13256d302bbd"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer of the chosen model (bert-base-uncased)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "max_length = 128\n",
    "\n",
    "# Define tokenization function for Hugging Face Datasets\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"],\n",
    "                     padding=\"max_length\",\n",
    "                     truncation=True,\n",
    "                     max_length=max_length)\n",
    "\n",
    "# Apply the tokenizer to both datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ROOT2BNwJPD"
   },
   "source": [
    "###Classic Fine-Tuning with Weighted Loss (No LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_ZVYPGZQl7Q"
   },
   "source": [
    "#### Define a Custom BERT Model with Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAuoMQkiQXVT"
   },
   "outputs": [],
   "source": [
    "# This custom class adds class weights support to the standard BERT classification head\n",
    "class BertWithWeightedLoss(BertForSequenceClassification):\n",
    "    def __init__(self, config, class_weights):\n",
    "        super().__init__(config)\n",
    "        self.class_weights = class_weights\n",
    "        self.loss_fn = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
    "        # Forward call to BERT base class\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Apply custom weighted loss if labels are provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpIjvAleQhfM"
   },
   "source": [
    " #### Load Model Configuration and Instantiate Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "37940038ad0948b28f3cfdc7431c9c18",
      "a5df36898d804ab98ff7f8af5dc45da2",
      "08c5b292c85f469e985bb27ee62c437e",
      "8e325804275b42b48fb7f2bc1daa27b2",
      "db0a667304574f06b71401af2e878f1c",
      "4c7bc14aaa13417cb984148128cf8782",
      "f718509afa4f4eaa90cafe5882ab409a",
      "eb6499778cb44d48a25bcef7e11ddc9a",
      "ac31dd705ce4412889144f4fb408474a",
      "bbdab5c28488451d948407ed63d62c5e",
      "7d7b82f7e28b4215a2e9ba963743189c"
     ]
    },
    "id": "V0GUDaL1QeK7",
    "outputId": "17a2a9a9-0630-4801-a099-396895913a48"
   },
   "outputs": [],
   "source": [
    "# Load the BERT config\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Instantiate the custom model with class weights\n",
    "model = BertWithWeightedLoss.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    config=config,\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klHK_ip6YuAN"
   },
   "source": [
    "Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VlxZh-6jwyss",
    "outputId": "b1cf2322-224b-432d-c4b4-48623bd1b569"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define the training configuration for the Hugging Face Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                 # Directory to save checkpoints\n",
    "    evaluation_strategy=\"epoch\",            # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                  # Save model at the end of each epoch\n",
    "    logging_strategy=\"epoch\",               # Log metrics at each epoch\n",
    "    num_train_epochs=3,                     # Number of training epochs\n",
    "    per_device_train_batch_size=8,          # Training batch size per device\n",
    "    per_device_eval_batch_size=16,          # Eval batch size per device\n",
    "    warmup_steps=0,                         # Warm-up steps for learning rate scheduler\n",
    "    weight_decay=0.01,                      # L2 regularization to reduce overfitting\n",
    "    logging_dir=\"./logs\",                   # Directory to store log files\n",
    "    load_best_model_at_end=True,            # Automatically load best checkpoint (by metric)\n",
    "    metric_for_best_model=\"f1\",             # Use f1_score to track best model\n",
    "    report_to=\"none\"                        # Avoid sending logs to external tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mNEQMdsY1pi"
   },
   "source": [
    "### Define Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHlMtkUcx4tt"
   },
   "outputs": [],
   "source": [
    "# Compute standard classification metrics during evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)  # Convert logits to predicted class\n",
    "\n",
    "    # Compute precision, recall, F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='binary'\n",
    "    )\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euKzcADfipAq"
   },
   "source": [
    "###  Metric Selection: We will use F1-Score\n",
    "\n",
    "In this binary sentiment classification task, we are dealing with **imbalanced classes**: most of the reviews are **positive**, and fewer are **negative**.\n",
    "\n",
    "Using **accuracy** alone can be misleading, since a model that predicts \"positive\" all the time may still achieve high accuracy but fail to detect negative reviews.\n",
    "\n",
    "To address this, we choose the **F1-score** as our main evaluation metric.  \n",
    "It provides a **balanced trade-off** between **precision** (avoiding false positives) and **recall** (avoiding false negatives), which is essential for this kind of task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5PLlD1eRAiu"
   },
   "source": [
    "### Train with Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eE9A9G7pxkOg",
    "outputId": "8b12e293-dbfd-4186-fe0b-73d1d32df3ca"
   },
   "outputs": [],
   "source": [
    "# Hugging Face Trainer takes care of training, evaluation, logging, saving\n",
    "trainer = Trainer(\n",
    "    model=model,                            # Our custom weighted model\n",
    "    args=training_args,                     # Training config\n",
    "    train_dataset=tokenized_train,          # Tokenized training dataset\n",
    "    eval_dataset=tokenized_test,            # Tokenized evaluation dataset\n",
    "    tokenizer=tokenizer,                    # Tokenizer used for preprocessing\n",
    "    compute_metrics=compute_metrics         # Custom evaluation metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "xNS4x34byAqk",
    "outputId": "0e266b68-a758-4941-ac87-296a7dd14403"
   },
   "outputs": [],
   "source": [
    "# Start the training process and monitor progress\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mGjdxiB3k5HL",
    "outputId": "54e782c0-abaa-4e01-bdb4-06bdf5ca404c"
   },
   "outputs": [],
   "source": [
    "# Show which step/checkpoint was considered the best (F1-wise)\n",
    "print(\"Best metric:\", trainer.state.best_metric)\n",
    "print(\"Best model checkpoint path:\", trainer.state.best_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKB-_BJ2lPa3",
    "outputId": "4303e1b2-a5e5-476a-ed23-30d59dc40d79"
   },
   "outputs": [],
   "source": [
    "# Extract the epoch number from the checkpoint folder name\n",
    "checkpoint_path = trainer.state.best_model_checkpoint\n",
    "\n",
    "# Extract step number from checkpoint path (e.g., \"./results/checkpoint-187\")\n",
    "match = re.search(r\"checkpoint-(\\d+)\", checkpoint_path)\n",
    "if match:\n",
    "    best_step = int(match.group(1))\n",
    "    print(\"Best step:\", best_step)\n",
    "else:\n",
    "    print(\"Unable to determine step from checkpoint path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fI3tdnhalh7N",
    "outputId": "9f09259a-95d4-4368-aeab-89dc846bb481"
   },
   "outputs": [],
   "source": [
    "# Nombre total d'exemples d'entraÃ®nement\n",
    "num_train_samples = len(tokenized_train)\n",
    "\n",
    "# Taille des batchs\n",
    "batch_size = training_args.per_device_train_batch_size\n",
    "\n",
    "# Nombre de steps par epoch\n",
    "steps_per_epoch = num_train_samples // batch_size\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goawcWPXllK3",
    "outputId": "ac71aa1a-8b6e-480c-ec44-f44218e49f8d"
   },
   "outputs": [],
   "source": [
    "best_epoch_approx = best_step / steps_per_epoch\n",
    "print(f\"Best model â‰ˆ epoch {best_epoch_approx:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaL1KzTLmWkc"
   },
   "source": [
    "### Training Analysis â€“ Full Fine-Tuning (Best Model at Epoch 3)\n",
    "\n",
    "We now visualize the training and validation curves for the best model obtained using full fine-tuning.\n",
    "\n",
    "This model was selected based on the **highest F1-score**, and it corresponds to approximately **epoch 3.0**.  \n",
    "We plot both the **training and validation losses**, as well as the **validation F1-score** to observe the model's performance and stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "xNjc8Z8KmFx-",
    "outputId": "161469c6-710a-47e2-89fa-69a00cdaab92"
   },
   "outputs": [],
   "source": [
    "# Plot Train Loss, Validation Loss, and Validation F1 for the best full fine-tuned model\n",
    "\n",
    "# Retrieve training logs (filtered up to epoch 3)\n",
    "logs = trainer.state.log_history\n",
    "logs = [log for log in logs if \"epoch\" in log and log[\"epoch\"] <= 3.0]\n",
    "\n",
    "# Extract training and evaluation metrics\n",
    "train_epochs = [log[\"epoch\"] for log in logs if \"loss\" in log and \"eval_loss\" not in log]\n",
    "train_losses = [log[\"loss\"] for log in logs if \"loss\" in log and \"eval_loss\" not in log]\n",
    "\n",
    "eval_epochs = [log[\"epoch\"] for log in logs if \"eval_loss\" in log]\n",
    "eval_losses = [log[\"eval_loss\"] for log in logs if \"eval_loss\" in log]\n",
    "eval_f1s = [log[\"eval_f1\"] for log in logs if \"eval_f1\" in log]\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Loss Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_epochs, train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(eval_epochs, eval_losses, label=\"Validation Loss\", marker='x')\n",
    "plt.title(\"Loss per Epoch (Full Fine-Tuning)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# F1 Score Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(eval_epochs, eval_f1s, label=\"Validation F1\", marker='o', color='purple')\n",
    "plt.title(\"Validation F1 Score per Epoch (Full Fine-Tuning)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wO1G0Vczp-o"
   },
   "source": [
    "### Fine-Tuning with LoRA (Parameter-Efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEio7LaOmt4G",
    "outputId": "6977e91a-a0a1-4390-80b4-83fb32507fb9"
   },
   "outputs": [],
   "source": [
    "# Reload the base model (unmodified) for LoRA training\n",
    "base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define LoRA config for sequence classification\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,    # Task: sequence classification\n",
    "    r=4,                           # Rank (low-rank decomposition)\n",
    "    lora_alpha=8,                  # Scaling factor\n",
    "    lora_dropout=0.1,              # Dropout for regularization\n",
    "    bias=\"lora_only\"               # Only apply LoRA to bias layers\n",
    ")\n",
    "\n",
    "# Apply LoRA to the base model\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Check number of trainable parameters\n",
    "lora_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "S-1KheeHnBN3",
    "outputId": "cd63b08f-7c2c-4fdd-d59c-ef506dc7b7a9"
   },
   "outputs": [],
   "source": [
    "# Retrain using the same Trainer API, but with the LoRA-augmented model\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwzJVpJRR2qu"
   },
   "source": [
    "#### Compare Fine-Tuning Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "XzeStZUsVgjP",
    "outputId": "e0072c37-1965-4dd5-fdcf-9479df7552d7"
   },
   "outputs": [],
   "source": [
    "# Simple performance comparison between Full Fine-tuning and LoRA\n",
    "data = {\n",
    "    \"Metric\": [\n",
    "        \"Accuracy max\",\n",
    "        \"F1 max\",\n",
    "        \"Training time (sec)\",\n",
    "        \"Training loss final\",\n",
    "        \"Validation loss final\"\n",
    "    ],\n",
    "    \"Full Fine-tuning\": [\n",
    "        0.9822,\n",
    "        0.9905,\n",
    "        339,\n",
    "        0.0739,\n",
    "        0.4286\n",
    "    ],\n",
    "    \"LoRA\": [\n",
    "        0.9382,\n",
    "        0.9678,\n",
    "        202,\n",
    "        0.1649,\n",
    "        0.1681\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(data).set_index(\"Metric\")\n",
    "df_comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OtqBezrypDZQ",
    "outputId": "174918bb-42ec-4efc-96c4-9c524d1917cd"
   },
   "outputs": [],
   "source": [
    "# Get best checkpoint path for LoRA\n",
    "lora_best_checkpoint = trainer.state.best_model_checkpoint\n",
    "print(\"ðŸ”Ž Best LoRA model checkpoint path:\", lora_best_checkpoint)\n",
    "\n",
    "# Extract step\n",
    "match = re.search(r\"checkpoint-(\\d+)\", lora_best_checkpoint)\n",
    "if match:\n",
    "    lora_best_step = int(match.group(1))\n",
    "    print(\"Best step (LoRA):\", lora_best_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPZN4GFcpIMs",
    "outputId": "8aee8c63-9224-4cb1-8ae7-38dc7c6b6907"
   },
   "outputs": [],
   "source": [
    "# Define your save path\n",
    "save_path = \"./app/bert_sentiment_lora\"\n",
    "\n",
    "# Save LoRA adapters\n",
    "lora_model.save_pretrained(save_path)\n",
    "\n",
    "# Save tokenizer (same tokenizer as before)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"LoRA model and tokenizer saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bZLfukCSELe"
   },
   "source": [
    "### Save Final Model for Reuse (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gv9WYV7L_il4",
    "outputId": "c240d05c-3a8e-4285-a742-f3abe145d292"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Function to load LoRA fine-tuned model and tokenizer\n",
    "def load_classification_model():\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    model = PeftModel.from_pretrained(base_model, \"app/bert_sentiment_lora\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"app/bert_sentiment_lora\")\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "# Load model (to be reused in Notebook 2 or Streamlit)\n",
    "cls_tokenizer, cls_model = load_classification_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fb4OPoYp_4B",
    "outputId": "30f77fc4-1328-4ab0-a662-99499d2fd934"
   },
   "outputs": [],
   "source": [
    "print(\"Model is ready for inference\")\n",
    "inputs = cls_tokenizer(\"Great product, fast delivery!\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = cls_model(**inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    print(\"Predicted probabilities:\", probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrmHLhiMqMX9"
   },
   "source": [
    "### Inference Check â€“ LoRA Model\n",
    "\n",
    "The model has been successfully loaded using the base BERT architecture combined with the LoRA adapters.\n",
    "\n",
    "We tested it on a simple sentence and verified that it produces a valid output, meaning the LoRA weights were correctly applied.\n",
    "\n",
    "This confirms that the model is now ready for:\n",
    "- Use in downstream tasks (in our case,  response generation)\n",
    "- Integration into a web application via Streamlit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKgz3xhwq3iR"
   },
   "source": [
    "### ðŸ“Š Training Analysis â€“ LoRA Fine-Tuning (Best Model)\n",
    "\n",
    "We now plot the training and validation curves for the best model obtained using LoRA fine-tuning.\n",
    "\n",
    "As before, the model was selected based on the highest **F1-score**. We visualize both the **training and validation losses**, as well as the **validation F1-score** over the 3 training epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "Tla7OKiUq7Is",
    "outputId": "8941eb16-384e-4220-feee-4e795cc6d66e"
   },
   "outputs": [],
   "source": [
    "# ðŸ“Š Plot training & validation loss + validation F1 for LoRA model\n",
    "\n",
    "# Retrieve logs after LoRA fine-tuning\n",
    "lora_logs = trainer.state.log_history\n",
    "lora_logs = [log for log in lora_logs if \"epoch\" in log and log[\"epoch\"] <= 3.0]\n",
    "\n",
    "# Extract metrics\n",
    "lora_train_epochs = [log[\"epoch\"] for log in lora_logs if \"loss\" in log and \"eval_loss\" not in log]\n",
    "lora_train_losses = [log[\"loss\"] for log in lora_logs if \"loss\" in log and \"eval_loss\" not in log]\n",
    "\n",
    "lora_eval_epochs = [log[\"epoch\"] for log in lora_logs if \"eval_loss\" in log]\n",
    "lora_eval_losses = [log[\"eval_loss\"] for log in lora_logs if \"eval_loss\" in log]\n",
    "lora_eval_f1s = [log[\"eval_f1\"] for log in lora_logs if \"eval_f1\" in log]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lora_train_epochs, lora_train_losses, label=\"Train Loss (LoRA)\", marker='o')\n",
    "plt.plot(lora_eval_epochs, lora_eval_losses, label=\"Validation Loss (LoRA)\", marker='x')\n",
    "plt.title(\"Loss per Epoch (LoRA Fine-Tuning)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# F1\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lora_eval_epochs, lora_eval_f1s, label=\"Validation F1 (LoRA)\", marker='o', color='purple')\n",
    "plt.title(\"Validation F1 Score per Epoch (LoRA Fine-Tuning)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzAoZZLHrpxp"
   },
   "source": [
    "### Model Selection Justification\n",
    "\n",
    "Based on the training curves, both models show good performance, but with different dynamics:\n",
    "\n",
    "- The **full fine-tuning** model achieved slightly higher F1 and accuracy scores overall. However, its **validation loss decreased and then increased**, which may indicate the beginning of overfitting.\n",
    "- In contrast, the **LoRA fine-tuned model** showed a **consistently decreasing loss** on both training and validation sets, and its **F1-score steadily increased** over the epochs.\n",
    "\n",
    "Despite having slightly lower peak performance, the LoRA model demonstrates **better generalization stability** and a **lighter memory footprint**, which makes it more suitable for deployment.\n",
    "\n",
    "We therefore choose the **LoRA model** as our final model for inference and response generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpbiY7CCsoLs"
   },
   "source": [
    "## Saving Results and Final Model\n",
    "\n",
    "To use our classifier in the next phase (response generation and app interface), we save:\n",
    "\n",
    "- A CSV file containing the test texts, true labels, and predicted sentiments.\n",
    "- The LoRA fine-tuned model and tokenizer, ready to be reloaded and used for inference.\n",
    "\n",
    "This ensures that the model is easily portable across notebooks and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvFSALTLsPS1"
   },
   "source": [
    "#### Save test_with_predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "u5Cf1KYTsGHH",
    "outputId": "d0b02470-1175-475d-d04c-bd92c74502ad"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "# Generate predictions on test set\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "predicted_probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=-1)\n",
    "predicted_labels = predicted_probs.argmax(dim=1).numpy()\n",
    "\n",
    "# Recreate the test texts from the original dataframe\n",
    "test_df_with_texts = test_df.copy()\n",
    "test_df_with_texts[\"predicted_label\"] = predicted_labels\n",
    "test_df_with_texts[\"predicted_sentiment\"] = test_df_with_texts[\"predicted_label\"].apply(lambda x: \"positive\" if x == 1 else \"negative\")\n",
    "\n",
    "# Save directly into app/ folder\n",
    "output_path = \"app/test_with_predictions.csv\"\n",
    "test_df_with_texts.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\" test_with_predictions.csv saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUjvh_79sXCD"
   },
   "source": [
    "#### Save LoRA model and tokenizer (local folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mlfFCQisbMY",
    "outputId": "95d1214b-5eb8-4502-96bb-d493559dbf2c"
   },
   "outputs": [],
   "source": [
    "# Define path\n",
    "save_path = \"./app/bert_sentiment_lora\"\n",
    "\n",
    "# Save model\n",
    "lora_model.save_pretrained(save_path)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\" LoRA model and tokenizer saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSaznS2ovyxh",
    "outputId": "f714f731-2ce0-4df0-e6ef-a81c0ad50a57"
   },
   "outputs": [],
   "source": [
    "# Compress the entire 'app/' directory into a single ZIP file\n",
    "shutil.make_archive(\"bert_sentiment_package\", format=\"zip\", root_dir=\"app\")\n",
    "\n",
    "print(\"All files in 'app/' have been compressed into 'bert_sentiment_package.zip'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "xpzrR_MHxKyF",
    "outputId": "3a1199ff-ead0-4016-91bf-076eb13092f7"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"bert_sentiment_package.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
