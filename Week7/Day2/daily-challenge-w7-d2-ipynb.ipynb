{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:11:07.123635Z","iopub.execute_input":"2025-03-31T06:11:07.124044Z","iopub.status.idle":"2025-03-31T06:11:08.322339Z","shell.execute_reply.started":"2025-03-31T06:11:07.124002Z","shell.execute_reply":"2025-03-31T06:11:08.321291Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Introduction :\n\nDans ce notebook, nous exploitons les capacités d’un **modèle de langage open source (LLM)** — ici **Gemma 3B**, un modèle compatible avec `llama.cpp` — pour effectuer de la génération de texte et de code **en local sur Kaggle**, sans avoir recours à une API externe.\n\nVoici ce que ce projet nous permettra d’expérimenter concrètement :\n\n-  **Chargement d’un LLM optimisé** au format GGUF (Gemma 3B) à l’aide de `llama-cpp-python`, une interface Python pour `llama.cpp`.\n-  **Initialisation manuelle du modèle** avec contrôle sur les paramètres (`n_ctx`, `n_threads`, `n_gpu_layers`), ce qui vous permet de comprendre l’impact des ressources matérielles.\n-  **Génération de texte à partir de prompts naturels**, pour explorer la capacité du modèle à produire des réponses cohérentes et informatives.\n-  **Génération de code Python** à partir d’instructions simples, testant les compétences du modèle dans des tâches proches de Copilot ou ChatGPT.\n-  **Streaming token par token**, pour simuler un affichage progressif des réponses comme dans une interface de chatbot.\n-  **Exercices pratiques** visant à tester les compétences du modèle sur des cas concrets : fonctions, scripts de scraping, explications en langage naturel, etc.","metadata":{}},{"cell_type":"markdown","source":"1. Install llama-cpp-python","metadata":{}},{"cell_type":"code","source":"# Install llama-cpp-python quietly\n!pip install llama-cpp-python > /dev/null 2>&1\nprint(\" llama-cpp-python installed successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:13:14.278572Z","iopub.execute_input":"2025-03-31T06:13:14.278997Z","iopub.status.idle":"2025-03-31T06:15:29.058330Z","shell.execute_reply.started":"2025-03-31T06:13:14.278953Z","shell.execute_reply":"2025-03-31T06:15:29.056924Z"}},"outputs":[{"name":"stdout","text":" llama-cpp-python installed successfully.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"2. Download a compatible model in GGUF format (Gemma 3B)","metadata":{}},{"cell_type":"code","source":"# Download GGUF model (Gemma 3B, Q8_0 quantized)\n!wget -q https://huggingface.co/MaziyarPanahi/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it.Q8_0.gguf -O gemma-3-1b-it.Q8_0.gguf\nprint(\" Model downloaded successfully (gemma-3-1b-it.Q8_0.gguf)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:17:05.390731Z","iopub.execute_input":"2025-03-31T06:17:05.391171Z","iopub.status.idle":"2025-03-31T06:17:09.706416Z","shell.execute_reply.started":"2025-03-31T06:17:05.391137Z","shell.execute_reply":"2025-03-31T06:17:09.704927Z"}},"outputs":[{"name":"stdout","text":" Model downloaded successfully (gemma-3-1b-it.Q8_0.gguf)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"3. Load the model with llama_cpp\npython\nCopier le code\n","metadata":{}},{"cell_type":"code","source":"from llama_cpp import Llama\n\nllm = Llama(\n    model_path=\"gemma-3-1b-it.Q8_0.gguf\",\n    n_ctx=2048,\n    n_threads=8,       # Adjust based on CPU\n    n_gpu_layers=0,    # Set to 0 if no GPU\n    verbose=False\n)\n\nprint(\"Model loaded successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:19:14.266177Z","iopub.execute_input":"2025-03-31T06:19:14.266769Z","iopub.status.idle":"2025-03-31T06:19:15.186436Z","shell.execute_reply.started":"2025-03-31T06:19:14.266735Z","shell.execute_reply":"2025-03-31T06:19:15.185247Z"}},"outputs":[{"name":"stderr","text":"llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"4. Quick prompt test (text generation)","metadata":{}},{"cell_type":"code","source":"prompt = \" What is the difference between a star and a planet?\"\noutput = llm(prompt, max_tokens=100)\nprint(\" Response:\", output[\"choices\"][0][\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:20:01.558633Z","iopub.execute_input":"2025-03-31T06:20:01.559027Z","iopub.status.idle":"2025-03-31T06:20:12.037136Z","shell.execute_reply.started":"2025-03-31T06:20:01.558990Z","shell.execute_reply":"2025-03-31T06:20:12.036117Z"}},"outputs":[{"name":"stdout","text":" Response: \n\nA star is a massive, luminous ball of plasma that produces its own light and heat through nuclear fusion. Planets are celestial bodies that orbit a star and are large enough to have cleared their orbital path.\n\nHere's a breakdown of the key differences:\n\n*   **Light and Heat:** Stars generate their own light and heat through nuclear fusion, while planets reflect light from a star.\n*   **Orbit:** Stars orbit a central point, while planets orbit stars.\n*   **\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"5. Code generation example","metadata":{}},{"cell_type":"code","source":"prompt_code = \"Write a Python script that loads a model from Hugging Face Transformers.\"\n\noutput = llm(prompt_code, max_tokens=120)\nprint(\" Generated Code:\\n\", output[\"choices\"][0][\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:23:33.181778Z","iopub.execute_input":"2025-03-31T06:23:33.182367Z","iopub.status.idle":"2025-03-31T06:23:47.102667Z","shell.execute_reply.started":"2025-03-31T06:23:33.182315Z","shell.execute_reply":"2025-03-31T06:23:47.101547Z"}},"outputs":[{"name":"stdout","text":" Generated Code:\n \n\n```python\nfrom transformers import pipeline\n\n# Load a pre-trained model\nmodel_name = \"bert-base-uncased\"  # Change this to your desired model\ntry:\n    model = pipeline(\"text-generation\", model=model_name)\n    print(f\"Model '{model_name}' loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n```\n\nKey improvements and explanations:\n\n* **Error Handling:** The `try...except` block is crucial.  It gracefully handles potential errors\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":" 6. Streamed response (token by token)","metadata":{}},{"cell_type":"code","source":"prompt_stream = \"Write a Python function that returns True if a number is prime.\"\n\noutput_stream = llm(prompt_stream, max_tokens=80, stream=True)\n\nprint(\" Streaming response:\\n\")\nfor chunk in output_stream:\n    print(chunk[\"choices\"][0][\"text\"], end=\"\", flush=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:24:29.595742Z","iopub.execute_input":"2025-03-31T06:24:29.596184Z","iopub.status.idle":"2025-03-31T06:24:38.250534Z","shell.execute_reply.started":"2025-03-31T06:24:29.596153Z","shell.execute_reply":"2025-03-31T06:24:38.249583Z"}},"outputs":[{"name":"stdout","text":" Streaming response:\n\n\n\n```python\ndef is_prime(n):\n  if n <= 1:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n```\n\n**Explanation:**\n\n1. **`def is_prime(n):","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"7. Bonus: Try your own prompts","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"bonus_prompts = [\n    \"Write a Python function to check if a number is prime.\",\n    \"Explain the difference between a list and a tuple in Python.\",\n    \"Create a script to read a CSV file and plot a line chart using matplotlib.\",\n    \"Generate a web scraper that extracts headlines from a news website using requests and BeautifulSoup.\",\n    \"Write a for loop that prints even numbers between 1 and 100.\"\n]\n\nfor i, p in enumerate(bonus_prompts, 1):\n    print(f\"\\n\\n--- Prompt {i} ---\\n {p}\")\n    out = llm(p, max_tokens=100)\n    print(out[\"choices\"][0][\"text\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T06:26:03.263644Z","iopub.execute_input":"2025-03-31T06:26:03.264057Z","iopub.status.idle":"2025-03-31T06:26:57.683064Z","shell.execute_reply.started":"2025-03-31T06:26:03.264025Z","shell.execute_reply":"2025-03-31T06:26:57.681980Z"}},"outputs":[{"name":"stdout","text":"\n\n--- Prompt 1 ---\n Write a Python function to check if a number is prime.\n\n\n```python\ndef is_prime(n):\n  \"\"\"\n  Check if a number is prime.\n\n  Args:\n    n: The number to check.\n\n  Returns:\n    True if the number is prime, False otherwise.\n  \"\"\"\n  if n <= 1:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      \n\n\n--- Prompt 2 ---\n Explain the difference between a list and a tuple in Python.\n\n\n**Python Lists**\n\n*   **Mutable:** Lists are mutable, meaning you can change their contents after they are created. You can add, remove, or modify elements.\n*   **Ordered:** Lists are ordered, meaning the elements are stored in a specific sequence.\n*   **Heterogeneous:** Lists can contain elements of different data types (e.g., integers, strings, floats).\n*   **Syntax:** You define a list using square brackets `[]`.\n\n**\n\n\n--- Prompt 3 ---\n Create a script to read a CSV file and plot a line chart using matplotlib.\n\n\n```python\nimport csv\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef plot_csv_data(csv_file, x_column, y_column, title=\"Line Chart\", xlabel=\"X-Axis\", ylabel=\"Y-Axis\"):\n    \"\"\"\n    Reads a CSV file, plots a line chart of the data in specified columns.\n\n    Args:\n        csv_file (str): Path to the CSV file.\n        x_column (\n\n\n--- Prompt 4 ---\n Generate a web scraper that extracts headlines from a news website using requests and BeautifulSoup.\n\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_headlines(url):\n  \"\"\"\n  Scrapes headlines from a news website.\n\n  Args:\n    url: The URL of the news website.\n\n  Returns:\n    A list of headlines.  Returns an empty list if there's an error.\n  \"\"\"\n  try:\n    response = requests.get(url)\n    response.raise_for_status()  #\n\n\n--- Prompt 5 ---\n Write a for loop that prints even numbers between 1 and 100.\n\n\n```python\nfor i in range(1, 101):\n    if i % 2 == 0:\n        print(i)\n```\n\n**Explanation:**\n\n1.  **`for i in range(1, 101):`**: This line initiates a `for` loop that iterates through the numbers from 1 to 100 (inclusive).  `range(1, 101)` generates a sequence of numbers starting\n","output_type":"stream"}],"execution_count":9}]}